# mpla_project/mpla/core/prompt_reviser.py

import json
import logging
from typing import Dict, Any

# Configure logging
logger = logging.getLogger(__name__)

REVISION_META_PROMPT = """
You are an AI Prompt Engineer. Your task is to revise a given prompt based on a critical analysis of its previous output.
Your goal is to improve the prompt to eliminate the identified flaws.

**Input:**
1.  **The Original Prompt:**
    ```
    {prompt}
    ```
2.  **Critical Analysis Report (of the output generated by the original prompt):**
    ```json
    {analysis_report}
    ```

**Instructions:**
- Carefully review the "Critical Analysis Report".
- Rewrite the "Original Prompt" to address the specific flaws mentioned in the report.
- Your revision should be surgical. Preserve the original intent and core instructions of the prompt while making targeted improvements.
- Do NOT add conversational text, apologies, or explanations.
- Your final output MUST be ONLY the revised prompt text.

**Revised Prompt:**
"""

class PromptReviser:
    """
    Revises a prompt based on analysis feedback.
    """

    def __init__(self, orchestrator: Any, temperature: float = 0.2):
        """
        Initializes the PromptReviser.

        Args:
            orchestrator: The deployment orchestrator to use for revision.
            temperature: The temperature to use for the revision model.
        """
        self.orchestrator = orchestrator
        self.temperature = temperature

    async def revise(self, prompt: str, analysis_report: Dict[str, Any]) -> str:
        """
        Revises the prompt based on the analysis report.

        Args:
            prompt: The original prompt to revise.
            analysis_report: The analysis report from the OutputAnalyzer.

        Returns:
            The revised prompt.
        """
        if not analysis_report.get("flaws_found"):
            logger.info("No flaws found in the analysis report. Prompt revision is not required.")
            return prompt

        logger.info("Revising prompt based on feedback...")
        
        # We convert the analysis dict to a JSON string for clean insertion into the prompt.
        analysis_str = json.dumps(analysis_report, indent=2)
        
        revision_prompt = REVISION_META_PROMPT.format(
            prompt=prompt,
            analysis_report=analysis_str
        )

        try:
            # We allow for some creativity in revision.
            revised_prompt = await self.orchestrator.generate_text(
                revision_prompt, temperature=self.temperature
            )

            if not revised_prompt:
                 raise ValueError("Revision generation returned an empty response.")

            logger.info("Prompt successfully revised.")
            return revised_prompt.strip()

        except Exception as e:
            logger.error("An unexpected error occurred during prompt revision: %s", e)
            # As a fallback, return the original prompt
            return prompt 