# mpla_project/mpla/core/output_analyzer.py

import json
import logging
from typing import Dict, Any

# Configure logging
logger = logging.getLogger(__name__)

ANALYSIS_META_PROMPT = """
You are an AI Critic. Your role is to analyze the output generated by a given prompt and identify potential flaws.
You must provide your analysis in a structured JSON format.

**Analysis Criteria:**
1.  **Factual Inaccuracy:** Does the output contain factually incorrect information?
2.  **Logical Inconsistency:** Does the output contradict itself or contain logical fallacies?
3.  **Constraint Adherence:** Does the output fail to follow any constraints, rules, or formatting instructions from the original prompt?
4.  **Ambiguity:** Is the output vague, unclear, or open to multiple interpretations where it should be specific?
5.  **Completeness:** Is the output missing required information or sections?

**Input:**
1.  **Original Prompt:**
    ```
    {prompt}
    ```
2.  **Generated Output:**
    ```
    {output}
    ```

**Instructions:**
- Analyze the "Generated Output" based on the "Original Prompt".
- Provide a concise, neutral analysis for each criterion.
- Your final output MUST be a JSON object with the following structure:
  {{
    "analysis_summary": {{
      "factual_inaccuracy": "<your analysis>",
      "logical_inconsistency": "<your analysis>",
      "constraint_adherence": "<your analysis>",
      "ambiguity": "<your analysis>",
      "completeness": "<your analysis>"
    }},
    "flaws_found": <true_or_false>,
    "feedback_summary": "<A brief, one-sentence summary of the most critical feedback. If no flaws are found, state that the output is high quality.>"
  }}
"""

class OutputAnalyzer:
    """
    Analyzes the output of a generated prompt to identify flaws.
    """

    def __init__(self, orchestrator: Any, temperature: float = 0.0):
        """
        Initializes the OutputAnalyzer.

        Args:
            orchestrator: The deployment orchestrator to use for analysis.
            temperature: The temperature to use for the analysis model.
        """
        self.orchestrator = orchestrator
        self.temperature = temperature

    async def analyze(self, prompt: str, output: str) -> Dict[str, Any]:
        """
        Analyzes the given prompt's output for flaws.

        Args:
            prompt: The prompt that was used to generate the output.
            output: The output generated by the prompt.

        Returns:
            A dictionary containing the analysis results.
        """
        logger.info("Analyzing prompt output with an AI critic...")
        analysis_prompt = ANALYSIS_META_PROMPT.format(prompt=prompt, output=output)

        try:
            # We use a low temperature for analysis to get consistent, predictable results.
            raw_analysis = await self.orchestrator.generate_text(
                analysis_prompt, temperature=self.temperature
            )
            
            if not raw_analysis:
                raise ValueError("Analysis generation returned an empty response.")

            # Clean the response to ensure it is valid JSON
            clean_json_str = raw_analysis.strip().replace("```json", "").replace("```", "")
            
            analysis_result = json.loads(clean_json_str)
            logger.info(f"Analysis successful: {analysis_result.get('feedback_summary')}")
            return analysis_result

        except json.JSONDecodeError:
            logger.error("Failed to decode JSON from analysis response: %s", raw_analysis)
            return {
                "flaws_found": True,
                "feedback_summary": "Error: The AI critic returned a response that was not valid JSON.",
                "analysis_summary": {}
            }
        except Exception as e:
            logger.error("An unexpected error occurred during output analysis: %s", e)
            return {
                "flaws_found": True,
                "feedback_summary": f"An unexpected error occurred: {e}",
                "analysis_summary": {}
            } 