*   **LLM-Based Evaluation:** Use a dedicated LLM call to score output on nuanced criteria (style, coherence, accuracy) for a more reliable learning signal.

*   **Proactive Error Recovery:** Implement a retry mechanism for self-correction failures (e.g., invalid JSON from LLM) by re-issuing the request with error-correcting instructions.

*   **Knowledge Base UI:** Develop a UI for browsing the `mpla_v2.db`, allowing users to review past sessions, compare prompt performance, and visualize agent improvement.

*   **Enhanced Self-Correction Log UI:** Redesign the self-correction log with collapsible sections, structured analysis tables, and a "diff" view to clearly show prompt modifications.

*   **Production Deployment Strategy:** Plan the transition to a live environment, including selecting a cloud host, containerizing the application with Docker, and establishing a CI/CD pipeline.

*   **Structured Prompt Input UI:** Replace the single text area with dedicated fields for 'Context', 'Objective', and 'Rules/Constraints' to improve prompt clarity and structure.

*   **Advanced LLM Integration:** Upgrade the system to support more powerful models like Gemini 2.5 Pro and add a model selection dropdown in the UI to balance cost, speed, and quality.
